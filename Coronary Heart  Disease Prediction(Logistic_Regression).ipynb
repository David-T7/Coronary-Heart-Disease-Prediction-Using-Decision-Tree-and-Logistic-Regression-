{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport copy\nimport math \n\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2023-01-30T22:48:10.414584Z","iopub.execute_input":"2023-01-30T22:48:10.414998Z","iopub.status.idle":"2023-01-30T22:48:10.427150Z","shell.execute_reply.started":"2023-01-30T22:48:10.414965Z","shell.execute_reply":"2023-01-30T22:48:10.425842Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"/kaggle/input/framingham-heart-study-dataset/framingham.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"ds = pd.read_csv('/kaggle/input/framingham-heart-study-dataset/framingham.csv')\n# drop nan rows\nds = ds.dropna()\nds.head(50)","metadata":{"execution":{"iopub.status.busy":"2023-01-30T22:48:13.782701Z","iopub.execute_input":"2023-01-30T22:48:13.783150Z","iopub.status.idle":"2023-01-30T22:48:13.890100Z","shell.execute_reply.started":"2023-01-30T22:48:13.783111Z","shell.execute_reply":"2023-01-30T22:48:13.888744Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"    male  age  education  currentSmoker  cigsPerDay  BPMeds  prevalentStroke  \\\n0      1   39        4.0              0         0.0     0.0                0   \n1      0   46        2.0              0         0.0     0.0                0   \n2      1   48        1.0              1        20.0     0.0                0   \n3      0   61        3.0              1        30.0     0.0                0   \n4      0   46        3.0              1        23.0     0.0                0   \n5      0   43        2.0              0         0.0     0.0                0   \n6      0   63        1.0              0         0.0     0.0                0   \n7      0   45        2.0              1        20.0     0.0                0   \n8      1   52        1.0              0         0.0     0.0                0   \n9      1   43        1.0              1        30.0     0.0                0   \n10     0   50        1.0              0         0.0     0.0                0   \n11     0   43        2.0              0         0.0     0.0                0   \n12     1   46        1.0              1        15.0     0.0                0   \n13     0   41        3.0              0         0.0     1.0                0   \n15     0   38        2.0              1        20.0     0.0                0   \n16     1   48        3.0              1        10.0     0.0                0   \n17     0   46        2.0              1        20.0     0.0                0   \n18     0   38        2.0              1         5.0     0.0                0   \n19     1   41        2.0              0         0.0     0.0                0   \n20     0   42        2.0              1        30.0     0.0                0   \n22     0   52        1.0              0         0.0     0.0                0   \n23     0   52        3.0              1        20.0     0.0                0   \n24     1   44        2.0              1        30.0     0.0                0   \n25     1   47        4.0              1        20.0     0.0                0   \n27     1   35        2.0              1        20.0     0.0                0   \n28     0   61        3.0              0         0.0     0.0                0   \n29     0   60        1.0              0         0.0     0.0                0   \n30     1   36        4.0              1        35.0     0.0                0   \n31     1   43        4.0              1        43.0     0.0                0   \n32     0   59        1.0              0         0.0     0.0                0   \n34     1   54        1.0              1        20.0     0.0                0   \n35     1   37        2.0              0         0.0     0.0                0   \n37     1   52        1.0              0         0.0     0.0                0   \n38     0   42        1.0              1         1.0     0.0                0   \n39     1   36        3.0              0         0.0     0.0                0   \n40     0   43        2.0              1        10.0     0.0                0   \n41     0   41        2.0              1         1.0     0.0                0   \n43     1   54        2.0              0         0.0     0.0                0   \n44     0   53        3.0              0         0.0     1.0                0   \n45     0   49        2.0              0         0.0     0.0                0   \n46     0   65        1.0              0         0.0     0.0                0   \n47     1   46        1.0              1        20.0     0.0                0   \n48     0   63        2.0              1        40.0     0.0                0   \n50     0   63        1.0              1         3.0     0.0                0   \n51     1   51        4.0              0         0.0     0.0                0   \n52     0   47        2.0              1        20.0     0.0                0   \n53     0   62        1.0              0         0.0     0.0                0   \n55     0   46        1.0              1        10.0     0.0                0   \n56     0   54        1.0              1         9.0     0.0                0   \n57     1   49        1.0              1         2.0     0.0                0   \n\n    prevalentHyp  diabetes  totChol  sysBP  diaBP    BMI  heartRate  glucose  \\\n0              0         0    195.0  106.0   70.0  26.97       80.0     77.0   \n1              0         0    250.0  121.0   81.0  28.73       95.0     76.0   \n2              0         0    245.0  127.5   80.0  25.34       75.0     70.0   \n3              1         0    225.0  150.0   95.0  28.58       65.0    103.0   \n4              0         0    285.0  130.0   84.0  23.10       85.0     85.0   \n5              1         0    228.0  180.0  110.0  30.30       77.0     99.0   \n6              0         0    205.0  138.0   71.0  33.11       60.0     85.0   \n7              0         0    313.0  100.0   71.0  21.68       79.0     78.0   \n8              1         0    260.0  141.5   89.0  26.36       76.0     79.0   \n9              1         0    225.0  162.0  107.0  23.61       93.0     88.0   \n10             0         0    254.0  133.0   76.0  22.91       75.0     76.0   \n11             0         0    247.0  131.0   88.0  27.64       72.0     61.0   \n12             1         0    294.0  142.0   94.0  26.31       98.0     64.0   \n13             1         0    332.0  124.0   88.0  31.31       65.0     84.0   \n15             1         0    221.0  140.0   90.0  21.35       95.0     70.0   \n16             1         0    232.0  138.0   90.0  22.37       64.0     72.0   \n17             0         0    291.0  112.0   78.0  23.38       80.0     89.0   \n18             0         0    195.0  122.0   84.5  23.24       75.0     78.0   \n19             0         0    195.0  139.0   88.0  26.88       85.0     65.0   \n20             0         0    190.0  108.0   70.5  21.59       72.0     85.0   \n22             0         0    234.0  148.0   78.0  34.17       70.0    113.0   \n23             0         0    215.0  132.0   82.0  25.11       71.0     75.0   \n24             1         0    270.0  137.5   90.0  21.96       75.0     83.0   \n25             0         0    294.0  102.0   68.0  24.18       62.0     66.0   \n27             1         0    225.0  132.0   91.0  26.09       73.0     83.0   \n28             1         0    272.0  182.0  121.0  32.80       85.0     65.0   \n29             0         0    247.0  130.0   88.0  30.36       72.0     74.0   \n30             0         0    295.0  102.0   68.0  28.15       60.0     63.0   \n31             0         0    226.0  115.0   85.5  27.57       75.0     75.0   \n32             1         0    209.0  150.0   85.0  20.77       90.0     88.0   \n34             1         0    214.0  147.0   74.0  24.71       96.0     87.0   \n35             1         0    225.0  124.5   92.5  38.53       95.0     83.0   \n37             1         1    178.0  160.0   98.0  40.11       75.0    225.0   \n38             1         0    233.0  153.0  101.0  28.93       60.0     90.0   \n39             0         0    180.0  111.0   73.0  27.78       71.0     80.0   \n40             0         0    243.0  116.5   80.0  26.87       68.0     78.0   \n41             0         0    237.0  122.0   78.0  23.28       75.0     74.0   \n43             0         0    195.0  132.0   83.5  26.21       75.0    100.0   \n44             1         1    311.0  206.0   92.0  21.51       76.0    215.0   \n45             0         0    208.0   96.0   63.0  20.68       65.0     98.0   \n46             1         0    252.0  179.5  114.0  30.47       90.0     87.0   \n47             0         0    261.0  119.0   77.5  23.59       75.0     74.0   \n48             0         0    179.0  116.0   69.0  22.15       95.0     75.0   \n50             1         0    267.0  156.5   92.5  27.10       60.0     79.0   \n51             0         0    216.0  112.0   66.0  23.47       90.0     95.0   \n52             0         0    237.0  130.0   78.0  19.66       80.0     75.0   \n53             0         0    240.0  145.0   82.5  28.27       63.0     75.0   \n55             0         0    250.0  116.0   71.0  20.35       88.0     94.0   \n56             0         1    266.0  114.0   76.0  17.61       88.0     55.0   \n57             1         0    255.0  143.5   81.0  25.65       75.0     80.0   \n\n    TenYearCHD  \n0            0  \n1            0  \n2            0  \n3            1  \n4            0  \n5            0  \n6            1  \n7            0  \n8            0  \n9            0  \n10           0  \n11           0  \n12           0  \n13           0  \n15           1  \n16           0  \n17           1  \n18           0  \n19           0  \n20           0  \n22           0  \n23           0  \n24           0  \n25           1  \n27           0  \n28           1  \n29           0  \n30           0  \n31           0  \n32           1  \n34           0  \n35           0  \n37           0  \n38           0  \n39           0  \n40           0  \n41           0  \n43           0  \n44           1  \n45           0  \n46           0  \n47           0  \n48           1  \n50           1  \n51           0  \n52           0  \n53           0  \n55           0  \n56           0  \n57           0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>male</th>\n      <th>age</th>\n      <th>education</th>\n      <th>currentSmoker</th>\n      <th>cigsPerDay</th>\n      <th>BPMeds</th>\n      <th>prevalentStroke</th>\n      <th>prevalentHyp</th>\n      <th>diabetes</th>\n      <th>totChol</th>\n      <th>sysBP</th>\n      <th>diaBP</th>\n      <th>BMI</th>\n      <th>heartRate</th>\n      <th>glucose</th>\n      <th>TenYearCHD</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>39</td>\n      <td>4.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>195.0</td>\n      <td>106.0</td>\n      <td>70.0</td>\n      <td>26.97</td>\n      <td>80.0</td>\n      <td>77.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>46</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>250.0</td>\n      <td>121.0</td>\n      <td>81.0</td>\n      <td>28.73</td>\n      <td>95.0</td>\n      <td>76.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>48</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>20.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>245.0</td>\n      <td>127.5</td>\n      <td>80.0</td>\n      <td>25.34</td>\n      <td>75.0</td>\n      <td>70.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>61</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>30.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>225.0</td>\n      <td>150.0</td>\n      <td>95.0</td>\n      <td>28.58</td>\n      <td>65.0</td>\n      <td>103.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>46</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>23.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>285.0</td>\n      <td>130.0</td>\n      <td>84.0</td>\n      <td>23.10</td>\n      <td>85.0</td>\n      <td>85.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>43</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>228.0</td>\n      <td>180.0</td>\n      <td>110.0</td>\n      <td>30.30</td>\n      <td>77.0</td>\n      <td>99.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>63</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>205.0</td>\n      <td>138.0</td>\n      <td>71.0</td>\n      <td>33.11</td>\n      <td>60.0</td>\n      <td>85.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0</td>\n      <td>45</td>\n      <td>2.0</td>\n      <td>1</td>\n      <td>20.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>313.0</td>\n      <td>100.0</td>\n      <td>71.0</td>\n      <td>21.68</td>\n      <td>79.0</td>\n      <td>78.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1</td>\n      <td>52</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>260.0</td>\n      <td>141.5</td>\n      <td>89.0</td>\n      <td>26.36</td>\n      <td>76.0</td>\n      <td>79.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1</td>\n      <td>43</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>30.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>225.0</td>\n      <td>162.0</td>\n      <td>107.0</td>\n      <td>23.61</td>\n      <td>93.0</td>\n      <td>88.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0</td>\n      <td>50</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>254.0</td>\n      <td>133.0</td>\n      <td>76.0</td>\n      <td>22.91</td>\n      <td>75.0</td>\n      <td>76.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0</td>\n      <td>43</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>247.0</td>\n      <td>131.0</td>\n      <td>88.0</td>\n      <td>27.64</td>\n      <td>72.0</td>\n      <td>61.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1</td>\n      <td>46</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>15.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>294.0</td>\n      <td>142.0</td>\n      <td>94.0</td>\n      <td>26.31</td>\n      <td>98.0</td>\n      <td>64.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0</td>\n      <td>41</td>\n      <td>3.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>332.0</td>\n      <td>124.0</td>\n      <td>88.0</td>\n      <td>31.31</td>\n      <td>65.0</td>\n      <td>84.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0</td>\n      <td>38</td>\n      <td>2.0</td>\n      <td>1</td>\n      <td>20.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>221.0</td>\n      <td>140.0</td>\n      <td>90.0</td>\n      <td>21.35</td>\n      <td>95.0</td>\n      <td>70.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>1</td>\n      <td>48</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>10.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>232.0</td>\n      <td>138.0</td>\n      <td>90.0</td>\n      <td>22.37</td>\n      <td>64.0</td>\n      <td>72.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0</td>\n      <td>46</td>\n      <td>2.0</td>\n      <td>1</td>\n      <td>20.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>291.0</td>\n      <td>112.0</td>\n      <td>78.0</td>\n      <td>23.38</td>\n      <td>80.0</td>\n      <td>89.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0</td>\n      <td>38</td>\n      <td>2.0</td>\n      <td>1</td>\n      <td>5.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>195.0</td>\n      <td>122.0</td>\n      <td>84.5</td>\n      <td>23.24</td>\n      <td>75.0</td>\n      <td>78.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>1</td>\n      <td>41</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>195.0</td>\n      <td>139.0</td>\n      <td>88.0</td>\n      <td>26.88</td>\n      <td>85.0</td>\n      <td>65.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0</td>\n      <td>42</td>\n      <td>2.0</td>\n      <td>1</td>\n      <td>30.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>190.0</td>\n      <td>108.0</td>\n      <td>70.5</td>\n      <td>21.59</td>\n      <td>72.0</td>\n      <td>85.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0</td>\n      <td>52</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>234.0</td>\n      <td>148.0</td>\n      <td>78.0</td>\n      <td>34.17</td>\n      <td>70.0</td>\n      <td>113.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0</td>\n      <td>52</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>20.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>215.0</td>\n      <td>132.0</td>\n      <td>82.0</td>\n      <td>25.11</td>\n      <td>71.0</td>\n      <td>75.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>1</td>\n      <td>44</td>\n      <td>2.0</td>\n      <td>1</td>\n      <td>30.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>270.0</td>\n      <td>137.5</td>\n      <td>90.0</td>\n      <td>21.96</td>\n      <td>75.0</td>\n      <td>83.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>1</td>\n      <td>47</td>\n      <td>4.0</td>\n      <td>1</td>\n      <td>20.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>294.0</td>\n      <td>102.0</td>\n      <td>68.0</td>\n      <td>24.18</td>\n      <td>62.0</td>\n      <td>66.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>1</td>\n      <td>35</td>\n      <td>2.0</td>\n      <td>1</td>\n      <td>20.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>225.0</td>\n      <td>132.0</td>\n      <td>91.0</td>\n      <td>26.09</td>\n      <td>73.0</td>\n      <td>83.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0</td>\n      <td>61</td>\n      <td>3.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>272.0</td>\n      <td>182.0</td>\n      <td>121.0</td>\n      <td>32.80</td>\n      <td>85.0</td>\n      <td>65.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>0</td>\n      <td>60</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>247.0</td>\n      <td>130.0</td>\n      <td>88.0</td>\n      <td>30.36</td>\n      <td>72.0</td>\n      <td>74.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>1</td>\n      <td>36</td>\n      <td>4.0</td>\n      <td>1</td>\n      <td>35.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>295.0</td>\n      <td>102.0</td>\n      <td>68.0</td>\n      <td>28.15</td>\n      <td>60.0</td>\n      <td>63.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>1</td>\n      <td>43</td>\n      <td>4.0</td>\n      <td>1</td>\n      <td>43.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>226.0</td>\n      <td>115.0</td>\n      <td>85.5</td>\n      <td>27.57</td>\n      <td>75.0</td>\n      <td>75.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>0</td>\n      <td>59</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>209.0</td>\n      <td>150.0</td>\n      <td>85.0</td>\n      <td>20.77</td>\n      <td>90.0</td>\n      <td>88.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>1</td>\n      <td>54</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>20.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>214.0</td>\n      <td>147.0</td>\n      <td>74.0</td>\n      <td>24.71</td>\n      <td>96.0</td>\n      <td>87.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>1</td>\n      <td>37</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>225.0</td>\n      <td>124.5</td>\n      <td>92.5</td>\n      <td>38.53</td>\n      <td>95.0</td>\n      <td>83.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>1</td>\n      <td>52</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>178.0</td>\n      <td>160.0</td>\n      <td>98.0</td>\n      <td>40.11</td>\n      <td>75.0</td>\n      <td>225.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>0</td>\n      <td>42</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>233.0</td>\n      <td>153.0</td>\n      <td>101.0</td>\n      <td>28.93</td>\n      <td>60.0</td>\n      <td>90.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>1</td>\n      <td>36</td>\n      <td>3.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>180.0</td>\n      <td>111.0</td>\n      <td>73.0</td>\n      <td>27.78</td>\n      <td>71.0</td>\n      <td>80.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>0</td>\n      <td>43</td>\n      <td>2.0</td>\n      <td>1</td>\n      <td>10.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>243.0</td>\n      <td>116.5</td>\n      <td>80.0</td>\n      <td>26.87</td>\n      <td>68.0</td>\n      <td>78.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>0</td>\n      <td>41</td>\n      <td>2.0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>237.0</td>\n      <td>122.0</td>\n      <td>78.0</td>\n      <td>23.28</td>\n      <td>75.0</td>\n      <td>74.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>1</td>\n      <td>54</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>195.0</td>\n      <td>132.0</td>\n      <td>83.5</td>\n      <td>26.21</td>\n      <td>75.0</td>\n      <td>100.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>0</td>\n      <td>53</td>\n      <td>3.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>311.0</td>\n      <td>206.0</td>\n      <td>92.0</td>\n      <td>21.51</td>\n      <td>76.0</td>\n      <td>215.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>0</td>\n      <td>49</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>208.0</td>\n      <td>96.0</td>\n      <td>63.0</td>\n      <td>20.68</td>\n      <td>65.0</td>\n      <td>98.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>0</td>\n      <td>65</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>252.0</td>\n      <td>179.5</td>\n      <td>114.0</td>\n      <td>30.47</td>\n      <td>90.0</td>\n      <td>87.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>1</td>\n      <td>46</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>20.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>261.0</td>\n      <td>119.0</td>\n      <td>77.5</td>\n      <td>23.59</td>\n      <td>75.0</td>\n      <td>74.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>0</td>\n      <td>63</td>\n      <td>2.0</td>\n      <td>1</td>\n      <td>40.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>179.0</td>\n      <td>116.0</td>\n      <td>69.0</td>\n      <td>22.15</td>\n      <td>95.0</td>\n      <td>75.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>0</td>\n      <td>63</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>267.0</td>\n      <td>156.5</td>\n      <td>92.5</td>\n      <td>27.10</td>\n      <td>60.0</td>\n      <td>79.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>1</td>\n      <td>51</td>\n      <td>4.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>216.0</td>\n      <td>112.0</td>\n      <td>66.0</td>\n      <td>23.47</td>\n      <td>90.0</td>\n      <td>95.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td>0</td>\n      <td>47</td>\n      <td>2.0</td>\n      <td>1</td>\n      <td>20.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>237.0</td>\n      <td>130.0</td>\n      <td>78.0</td>\n      <td>19.66</td>\n      <td>80.0</td>\n      <td>75.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>53</th>\n      <td>0</td>\n      <td>62</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>240.0</td>\n      <td>145.0</td>\n      <td>82.5</td>\n      <td>28.27</td>\n      <td>63.0</td>\n      <td>75.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>55</th>\n      <td>0</td>\n      <td>46</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>10.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>250.0</td>\n      <td>116.0</td>\n      <td>71.0</td>\n      <td>20.35</td>\n      <td>88.0</td>\n      <td>94.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>56</th>\n      <td>0</td>\n      <td>54</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>266.0</td>\n      <td>114.0</td>\n      <td>76.0</td>\n      <td>17.61</td>\n      <td>88.0</td>\n      <td>55.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>57</th>\n      <td>1</td>\n      <td>49</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>255.0</td>\n      <td>143.5</td>\n      <td>81.0</td>\n      <td>25.65</td>\n      <td>75.0</td>\n      <td>80.0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# First Change the pandas dataframe to numpy array\n# ds = ds.to_numpy() \nm,n = ds.shape\ntrd = ds[: int(m*0.7),:]\ntsd = ds[int(m*0.7):,:]\nprint(trd.shape[1])\n","metadata":{"execution":{"iopub.status.busy":"2023-01-30T22:48:22.845133Z","iopub.execute_input":"2023-01-30T22:48:22.845706Z","iopub.status.idle":"2023-01-30T22:48:22.855260Z","shell.execute_reply.started":"2023-01-30T22:48:22.845657Z","shell.execute_reply":"2023-01-30T22:48:22.853657Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"16\n","output_type":"stream"}]},{"cell_type":"code","source":"X_train = trd[:,:n-1]\nY_train = trd[:,n-1:]\nX_test = tsd[:,:n-1]\nY_test = tsd[:,n-1:]\nprint(Y_train.shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-01-30T22:48:29.238613Z","iopub.execute_input":"2023-01-30T22:48:29.239022Z","iopub.status.idle":"2023-01-30T22:48:29.246456Z","shell.execute_reply.started":"2023-01-30T22:48:29.238990Z","shell.execute_reply":"2023-01-30T22:48:29.245012Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"(2560, 1)\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name=\"2.3\"></a>\n### 2.3  Sigmoid function\n\nRecall that for logistic regression, the model is represented as\n\n$$ f_{\\mathbf{w},b}(x) = g(\\mathbf{w}\\cdot \\mathbf{x} + b)$$\nwhere function $g$ is the sigmoid function. The sigmoid function is defined as:\n\n$$g(z) = \\frac{1}{1+e^{-z}}$$\n\nLet's implement the sigmoid function first, so it can be used by the rest of this project.\n\n<a name='ex-01'></a>\n\n- `z` is not always a single number, but can also be an array of numbers. \n- If the input is an array of numbers, we'd like to apply the sigmoid function to each value in the input array.\n\n","metadata":{}},{"cell_type":"code","source":"def sigmoid(z):\n    \"\"\"\n    Compute the sigmoid of z\n\n    Args:\n        z (ndarray): A scalar, numpy array of any size.\n\n    Returns:\n        g (ndarray): sigmoid(z), with the same shape as z\n         \n    \"\"\"\n          \n    g = 1/(1+np.exp(-z))\n    \n    return g","metadata":{"execution":{"iopub.status.busy":"2023-01-30T22:48:41.519632Z","iopub.execute_input":"2023-01-30T22:48:41.520106Z","iopub.status.idle":"2023-01-30T22:48:41.526099Z","shell.execute_reply.started":"2023-01-30T22:48:41.520069Z","shell.execute_reply":"2023-01-30T22:48:41.525266Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#testing the sigmoid function\nprint (\"sigmoid(0) = \" + str(sigmoid(0)))","metadata":{"execution":{"iopub.status.busy":"2023-01-30T22:48:44.541894Z","iopub.execute_input":"2023-01-30T22:48:44.542333Z","iopub.status.idle":"2023-01-30T22:48:44.549339Z","shell.execute_reply.started":"2023-01-30T22:48:44.542298Z","shell.execute_reply":"2023-01-30T22:48:44.547749Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"sigmoid(0) = 0.5\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a name=\"2.4\"></a>\n### 2.4 Cost function for logistic regression\n\nIn this section, we will implement the cost function for logistic regression.\n\n<a name='ex-02'></a>\n\n\nRecall that for logistic regression, the cost function is of the form \n\n$$ J(\\mathbf{w},b) = \\frac{1}{m}\\sum_{i=0}^{m-1} \\left[ loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) \\right] \\tag{1}$$\n\nwhere\n* m is the number of training examples in the dataset\n\n\n* $loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ is the cost for a single data point, which is - \n\n    $$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\tag{2}$$\n    \n    \n*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$, which is the actual label\n\n*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot \\mathbf{x^{(i)}} + b)$ where function $g$ is the sigmoid function.\n    * It might be helpful to first calculate an intermediate variable $z_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x^{(i)}} + b = w_0x^{(i)}_0 + ... + w_{n-1}x^{(i)}_{n-1} + b$ where $n$ is the number of features, before calculating $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(z_{\\mathbf{w},b}(\\mathbf{x}^{(i)}))$\n\n","metadata":{}},{"cell_type":"code","source":"def compute_cost(X, y, w, b, lambda_= 1):\n    \"\"\"\n    Computes the cost over all examples\n    Args:\n      X : (ndarray Shape (m,n)) data, m examples by n features\n      y : (array_like Shape (m,)) target value \n      w : (array_like Shape (n,)) Values of parameters of the model      \n      b : scalar Values of bias parameter of the model\n      lambda_: unused placeholder\n    Returns:\n      total_cost: (scalar)         cost \n    \"\"\"\n\n    m, n = X.shape\n    total_cost = 0\n    for i in range(m):\n        loss = -y[i]*np.log(sigmoid(np.dot(w,X[i])+b))-((1-y[i])*np.log(1-sigmoid(np.dot(w,X[i])+b)))\n        total_cost += loss\n    total_cost/=m\n    return total_cost","metadata":{"execution":{"iopub.status.busy":"2023-01-30T22:48:54.381010Z","iopub.execute_input":"2023-01-30T22:48:54.381416Z","iopub.status.idle":"2023-01-30T22:48:54.389880Z","shell.execute_reply.started":"2023-01-30T22:48:54.381385Z","shell.execute_reply":"2023-01-30T22:48:54.388452Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"m,n = X_train.shape\n\n# Compute and display cost with w initialized to zeroes\ninitial_w = np.zeros(n)\nprint(len(initial_w))\ninitial_b = 0.\ncost = compute_cost(X_train, Y_train, initial_w, initial_b)\nprint (cost)","metadata":{"execution":{"iopub.status.busy":"2023-01-30T22:48:57.772755Z","iopub.execute_input":"2023-01-30T22:48:57.773176Z","iopub.status.idle":"2023-01-30T22:48:57.842474Z","shell.execute_reply.started":"2023-01-30T22:48:57.773142Z","shell.execute_reply":"2023-01-30T22:48:57.841150Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"15\n[0.69314718]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a name=\"2.5\"></a>\n### 2.5 Gradient for logistic regression\n\nIn this section, we will implement the gradient for logistic regression.\n\nRecall that the gradient descent algorithm is:\n\n$$\\begin{align*}& \\text{repeat until convergence:} \\; \\lbrace \\newline \\; & b := b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\newline       \\; & w_j := w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; & \\text{for j := 0..n-1}\\newline & \\rbrace\\end{align*}$$\n\nwhere, parameters $b$, $w_j$ are all updated simultaniously","metadata":{}},{"cell_type":"code","source":"def compute_gradient(X, y, w, b, lambda_=None): \n    \"\"\"\n    Computes the gradient for logistic regression \n \n    Args:\n      X : (ndarray Shape (m,n)) variable such as house size \n      y : (array_like Shape (m,1)) actual value \n      w : (array_like Shape (n,1)) values of parameters of the model      \n      b : (scalar)                 value of parameter of the model \n      lambda_: unused placeholder.\n    Returns\n      dj_dw: (array_like Shape (n,1)) The gradient of the cost w.r.t. the parameters w. \n      dj_db: (scalar)                The gradient of the cost w.r.t. the parameter b. \n    \"\"\"\n    m,n = X.shape\n    dj_dw = np.zeros(w.shape)\n    dj_db = 0.\n    test_w=np.zeros(w.shape)\n    test_d=0\n\n    for i in range(m):\n        test_d += sigmoid(np.dot(w,X[i])+b)-y[i]\n        for j in range(n):\n            test_w[j] += (sigmoid(np.dot(w,X[i])+b)-y[i])*X[i,j]\n            \n    dj_dw = test_w/m\n    dj_db = test_d / m\n        \n    return dj_db, dj_dw","metadata":{"execution":{"iopub.status.busy":"2023-01-30T22:49:01.255193Z","iopub.execute_input":"2023-01-30T22:49:01.255622Z","iopub.status.idle":"2023-01-30T22:49:01.265085Z","shell.execute_reply.started":"2023-01-30T22:49:01.255589Z","shell.execute_reply":"2023-01-30T22:49:01.263613Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Compute and display gradient with w initialized to zeroes\ninitial_w = np.zeros(n)\ninitial_b = 0.\n\ndj_db, dj_dw = compute_gradient(X_train, Y_train, initial_w, initial_b)\nprint(f'dj_db at initial w (zeros):{dj_db}' )\nprint(f'dj_dw at initial w (zeros):{dj_dw.tolist()}' )","metadata":{"execution":{"iopub.status.busy":"2023-01-30T22:49:04.438246Z","iopub.execute_input":"2023-01-30T22:49:04.439384Z","iopub.status.idle":"2023-01-30T22:49:04.843772Z","shell.execute_reply.started":"2023-01-30T22:49:04.439336Z","shell.execute_reply":"2023-01-30T22:49:04.842386Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"dj_db at initial w (zeros):[0.34726563]\ndj_dw at initial w (zeros):[0.1365234375, 16.5138671875, 0.7115234375, 0.165234375, 2.8439453125, 0.0064453125, 0.000390625, 0.07734375, 0.0021484375, 80.997265625, 44.17685546875, 28.111328125, 8.811068359375005, 26.121875, 27.1904296875]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a name=\"2.6\"></a>\n### 2.6 Learning parameters using gradient descent \n\n\n- A good way to verify that gradient descent is working correctly is to look\nat the value of $J(\\mathbf{w},b)$ and check that it is decreasing with each step. \n\n- The value of $J(\\mathbf{w},b)$ should never increase, and should converge to a steady value by the end of the algorithm.","metadata":{}},{"cell_type":"code","source":"def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters, lambda_): \n    \"\"\"\n    Performs batch gradient descent to learn theta. Updates theta by taking \n    num_iters gradient steps with learning rate alpha\n    \n    Args:\n      X :    (array_like Shape (m, n)\n      y :    (array_like Shape (m,))\n      w_in : (array_like Shape (n,))  Initial values of parameters of the model\n      b_in : (scalar)                 Initial value of parameter of the model\n      cost_function:                  function to compute cost\n      alpha : (float)                 Learning rate\n      num_iters : (int)               number of iterations to run gradient descent\n      lambda_ (scalar, float)         regularization constant\n      \n    Returns:\n      w : (array_like Shape (n,)) Updated values of parameters of the model after\n          running gradient descent\n      b : (scalar)                Updated value of parameter of the model after\n          running gradient descent\n    \"\"\"\n    \n    # number of training examples\n    m = len(X)\n    \n    # An array to store cost J and w's at each iteration primarily for graphing later\n    J_history = []\n    w_history = []\n    \n    for i in range(num_iters):\n\n        # Calculate the gradient and update the parameters\n        dj_db, dj_dw = gradient_function(X, y, w_in, b_in, lambda_)   \n\n        # Update Parameters using w, b, alpha and gradient\n        w_in = w_in - alpha * dj_dw               \n        b_in = b_in - alpha * dj_db              \n       \n        # Save cost J at each iteration\n        if i<100000:      # prevent resource exhaustion \n            cost =  cost_function(X, y, w_in, b_in, lambda_)\n            J_history.append(cost)\n\n        # Print cost every at intervals 10 times or as many iterations if < 10\n        if i% math.ceil(num_iters/10) == 0 or i == (num_iters-1):\n            w_history.append(w_in)\n            print(f\"Iteration {i:4}: Cost {float(J_history[-1]):8.2f}   \")\n        \n    return w_in, b_in, J_history, w_history #return w and J,w history for graphing","metadata":{"execution":{"iopub.status.busy":"2023-01-30T22:49:08.043789Z","iopub.execute_input":"2023-01-30T22:49:08.044238Z","iopub.status.idle":"2023-01-30T22:49:08.055579Z","shell.execute_reply.started":"2023-01-30T22:49:08.044200Z","shell.execute_reply":"2023-01-30T22:49:08.054074Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"Now let's run the gradient descent algorithm above to learn the parameters for our dataset.\n\n**Note**\n\nThe code block below takes a couple of minutes to run, especially with a non-vectorized version. You can reduce the `iterations` to test your implementation and iterate faster. If you have time, try running 100,000 iterations for better results.","metadata":{}},{"cell_type":"code","source":"np.random.seed(1)\nintial_w = 0.01 * (np.random.rand(n).reshape(-1,1) - 0.5)\nprint(len(intial_w))\ninitial_b = -8\n\n# Some gradient descent settings\niterations = 1000\nalpha = 0.001\n\nw,b, J_history,_ = gradient_descent(X_train ,Y_train, initial_w, initial_b, \n                                   compute_cost, compute_gradient, alpha, iterations, 0)","metadata":{"execution":{"iopub.status.busy":"2023-01-30T22:49:13.224950Z","iopub.execute_input":"2023-01-30T22:49:13.225361Z","iopub.status.idle":"2023-01-30T22:58:28.297406Z","shell.execute_reply.started":"2023-01-30T22:49:13.225327Z","shell.execute_reply":"2023-01-30T22:58:28.296214Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"15\nIteration    0: Cost     6.16   \nIteration  100: Cost     3.15   \nIteration  200: Cost     5.58   \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:17: RuntimeWarning: divide by zero encountered in log\n  app.launch_new_instance()\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in multiply\n  app.launch_new_instance()\n","output_type":"stream"},{"name":"stdout","text":"Iteration  300: Cost      nan   \nIteration  400: Cost     8.34   \nIteration  500: Cost     2.12   \nIteration  600: Cost     0.68   \nIteration  700: Cost     8.85   \nIteration  800: Cost     3.15   \nIteration  900: Cost     1.10   \nIteration  999: Cost     0.71   \n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Evaluating logistic regression\n\nWe can evaluate the quality of the parameters we have found by seeing how well the learned model predicts on our training set. \n\nWe will implement the `predict` function below to do this.\n","metadata":{}},{"cell_type":"markdown","source":"To Complete the `predict` function to produce `1` or `0` predictions given a dataset and a learned parameter vector $w$ and $b$.\n- First we need to compute the prediction from the model $f(x^{(i)}) = g(w \\cdot x^{(i)})$ for every example \n    - we've implemented this before in the parts above\n- We interpret the output of the model ($f(x^{(i)})$) as the probability that $y^{(i)}=1$ given $x^{(i)}$ and parameterized by $w$.\n- Therefore, to get a final prediction ($y^{(i)}=0$ or $y^{(i)}=1$) from the logistic regression model, we can use the following heuristic -\n\n  if $f(x^{(i)}) >= 0.5$, predict $y^{(i)}=1$\n  \n  if $f(x^{(i)}) < 0.5$, predict $y^{(i)}=0$","metadata":{}},{"cell_type":"code","source":"def predict(X, w, b): \n    \"\"\"\n    Predict whether the label is 0 or 1 using learned logistic\n    regression parameters w\n    \n    Args:\n    X : (ndarray Shape (m, n))\n    w : (array_like Shape (n,))      Parameters of the model\n    b : (scalar, float)              Parameter of the model\n\n    Returns:\n    p: (ndarray (m,1))\n        The predictions for X using a threshold at 0.5\n    \"\"\"\n    # number of training examples\n    m, n = X.shape   \n    p = np.zeros(m)\n   \n    # Loop over each example\n    for i in range(m):\n        p[i] = sigmoid(np.dot(X[i],w)+b)>0.5\n       \n    return p","metadata":{"execution":{"iopub.status.busy":"2023-01-30T22:59:33.082942Z","iopub.execute_input":"2023-01-30T22:59:33.083442Z","iopub.status.idle":"2023-01-30T22:59:33.091385Z","shell.execute_reply.started":"2023-01-30T22:59:33.083399Z","shell.execute_reply":"2023-01-30T22:59:33.089907Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"#Compute accuracy on our training set\np = predict(X_train, w,b)\nprint('Train Accuracy: %f'%(np.mean(p == Y_train) * 100))","metadata":{"execution":{"iopub.status.busy":"2023-01-30T22:59:36.314895Z","iopub.execute_input":"2023-01-30T22:59:36.315964Z","iopub.status.idle":"2023-01-30T22:59:36.383359Z","shell.execute_reply.started":"2023-01-30T22:59:36.315921Z","shell.execute_reply":"2023-01-30T22:59:36.381836Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Train Accuracy: 79.707489\n","output_type":"stream"}]},{"cell_type":"code","source":"#Compute accuracy on our test set\np = predict(X_test, w,b)\nprint('Test Accuracy: %f'%(np.mean(p == Y_test) * 100))","metadata":{"execution":{"iopub.status.busy":"2023-01-30T22:59:39.714673Z","iopub.execute_input":"2023-01-30T22:59:39.715063Z","iopub.status.idle":"2023-01-30T22:59:39.735453Z","shell.execute_reply.started":"2023-01-30T22:59:39.715033Z","shell.execute_reply":"2023-01-30T22:59:39.734514Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Test Accuracy: 80.306966\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a name=\"3\"></a>\n## 3 - Regularized Logistic Regression\nour model  more susceptible to overfitting so we  will implement regularized logistic regression to fit data well.\n\n<a name=\"3.4\"></a>\n### 3.4 Cost function for regularized logistic regression\n\nIn this part, we will implement the cost function for regularized logistic regression.\n\nRecall that for regularized logistic regression, the cost function is of the form\n$$J(\\mathbf{w},b) = \\frac{1}{m}  \\sum_{i=0}^{m-1} \\left[ -y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\right] + \\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2$$\n\nCompare this to the cost function without regularization (which you implemented above), which is of the form \n\n$$ J(\\mathbf{w}.b) = \\frac{1}{m}\\sum_{i=0}^{m-1} \\left[ (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\\right]$$\n\nThe difference is the regularization term, which is $$\\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2$$ \nNote that the $b$ parameter is not regularized.\n\n","metadata":{}},{"cell_type":"code","source":"def compute_cost_reg(X, y, w, b, lambda_ = 1):\n    \"\"\"\n    Computes the cost over all examples\n    Args:\n      X : (array_like Shape (m,n)) data, m examples by n features\n      y : (array_like Shape (m,)) target value \n      w : (array_like Shape (n,)) Values of parameters of the model      \n      b : (array_like Shape (n,)) Values of bias parameter of the model\n      lambda_ : (scalar, float)    Controls amount of regularization\n    Returns:\n      total_cost: (scalar)         cost \n    \"\"\"\n\n    m, n = X.shape\n    \n    # Calls the compute_cost function that you implemented above\n    cost_without_reg = compute_cost(X, y, w, b) \n    \n    # You need to calculate this value\n    reg_cost = 0.\n    \n    ### START CODE HERE ###\n    for i in range(n):\n        reg_cost += w[i]**2\n\n        \n    ### END CODE HERE ### \n    \n    # Add the regularization cost to get the total cost\n    total_cost = cost_without_reg + (lambda_/(2 * m)) * reg_cost\n\n    return total_cost","metadata":{"execution":{"iopub.status.busy":"2023-01-30T22:59:44.190445Z","iopub.execute_input":"2023-01-30T22:59:44.190880Z","iopub.status.idle":"2023-01-30T22:59:44.198969Z","shell.execute_reply.started":"2023-01-30T22:59:44.190844Z","shell.execute_reply":"2023-01-30T22:59:44.197818Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"np.random.seed(1)\nintial_w = np.random.rand(n) - 0.5\nprint(len(intial_w))\ninitial_b = 0.\nlambda_ = 0.8\ncost = compute_cost_reg(X_train, Y_train, initial_w, initial_b, lambda_)\n\nprint(\"Regularized cost :\", cost)","metadata":{"execution":{"iopub.status.busy":"2023-01-30T22:59:47.438377Z","iopub.execute_input":"2023-01-30T22:59:47.438804Z","iopub.status.idle":"2023-01-30T22:59:47.566626Z","shell.execute_reply.started":"2023-01-30T22:59:47.438767Z","shell.execute_reply":"2023-01-30T22:59:47.565508Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"15\nRegularized cost : [0.69314718]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 3.5 Gradient for regularized logistic regression\n\nIn this section, we will implement the gradient for regularized logistic regression.\n\n\nThe gradient of the regularized cost function has two components. The first, $\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$ is a scalar, the other is a vector with the same shape as the parameters $\\mathbf{w}$, where the $j^\\mathrm{th}$ element is defined as follows:\n\n$$\\frac{\\partial J(\\mathbf{w},b)}{\\partial b} = \\frac{1}{m}  \\sum_{i=0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})  $$\n\n$$\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} = \\left( \\frac{1}{m}  \\sum_{i=0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) x_j^{(i)} \\right) + \\frac{\\lambda}{m} w_j  \\quad\\, \\mbox{for $j=0...(n-1)$}$$\n\nCompare this to the gradient of the cost function without regularization (which we implemented above), which is of the form \n$$\n\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)}) \\tag{2}\n$$\n$$\n\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)})x_{j}^{(i)} \\tag{3}\n$$\n\n\nAs you can see,$\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$ is the same, the difference is the following term in $\\frac{\\partial J(\\mathbf{w},b)}{\\partial w}$, which is $$\\frac{\\lambda}{m} w_j  \\quad\\, \\mbox{for $j=0...(n-1)$}$$ ","metadata":{}},{"cell_type":"code","source":"def compute_gradient_reg(X, y, w, b, lambda_ = 1): \n    \"\"\"\n    Computes the gradient for linear regression \n \n    Args:\n      X : (ndarray Shape (m,n))   variable such as house size \n      y : (ndarray Shape (m,))    actual value \n      w : (ndarray Shape (n,))    values of parameters of the model      \n      b : (scalar)                value of parameter of the model  \n      lambda_ : (scalar,float)    regularization constant\n    Returns\n      dj_db: (scalar)             The gradient of the cost w.r.t. the parameter b. \n      dj_dw: (ndarray Shape (n,)) The gradient of the cost w.r.t. the parameters w. \n\n    \"\"\"\n    m, n = X.shape\n    \n    dj_db, dj_dw = compute_gradient(X, y, w, b)\n\n    ### START CODE HERE ###     \n    for i in range(n):\n        dj_dw[i]+= (lambda_*w[i])/m\n    \n        \n    ### END CODE HERE ###         \n        \n    return dj_db, dj_dw","metadata":{"execution":{"iopub.status.busy":"2023-01-30T22:59:56.161210Z","iopub.execute_input":"2023-01-30T22:59:56.161634Z","iopub.status.idle":"2023-01-30T22:59:56.169198Z","shell.execute_reply.started":"2023-01-30T22:59:56.161603Z","shell.execute_reply":"2023-01-30T22:59:56.168098Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"np.random.seed(1) \nintial_w = np.random.rand(n) - 0.5\ninitial_b = 0.5\nlambda_ = 0.5\ndj_db, dj_dw = compute_gradient_reg(X_train, Y_train, initial_w, initial_b, lambda_)\n\nprint(f\"dj_db: {dj_db}\", )\nprint(f\"First few elements of regularized dj_dw:\\n {dj_dw.tolist()}\", )","metadata":{"execution":{"iopub.status.busy":"2023-01-30T23:00:01.299453Z","iopub.execute_input":"2023-01-30T23:00:01.299865Z","iopub.status.idle":"2023-01-30T23:00:01.704620Z","shell.execute_reply.started":"2023-01-30T23:00:01.299833Z","shell.execute_reply":"2023-01-30T23:00:01.703361Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"dj_db: [0.46972496]\nFirst few elements of regularized dj_dw:\n [0.19043424463457032, 22.579478771768255, 0.95342845229989, 0.22512464166591153, 3.9492842836254667, 0.009937316866302903, 0.0008689817625072442, 0.11455990612306552, 0.00506641375129419, 110.04557569960483, 60.36485491524335, 38.25330469664998, 11.968775493983498, 35.3558826007348, 37.1951178683103]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Initialize fitting parameters\nnp.random.seed(1)\ninitial_w = np.random.rand(n) - 0.5\ninitial_b = 1.\n\n# Set regularization parameter lambda_ to 1 (you can try varying this)\nlambda_ = 0.01;                                          \n# Some gradient descent settings\niterations = 1000\nalpha = 0.01\n\nw,b, J_history,_ = gradient_descent(X_train, Y_train, initial_w, initial_b, \n                                    compute_cost_reg, compute_gradient_reg, \n                                    alpha, iterations, lambda_)","metadata":{"execution":{"iopub.status.busy":"2023-01-30T23:00:46.903733Z","iopub.execute_input":"2023-01-30T23:00:46.904592Z","iopub.status.idle":"2023-01-30T23:10:17.123052Z","shell.execute_reply.started":"2023-01-30T23:00:46.904527Z","shell.execute_reply":"2023-01-30T23:10:17.121847Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: overflow encountered in exp\n  del sys.path[0]\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:17: RuntimeWarning: divide by zero encountered in log\n  app.launch_new_instance()\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in multiply\n  app.launch_new_instance()\n","output_type":"stream"},{"name":"stdout","text":"Iteration    0: Cost      nan   \nIteration  100: Cost      nan   \nIteration  200: Cost      nan   \nIteration  300: Cost      nan   \nIteration  400: Cost      nan   \nIteration  500: Cost      nan   \nIteration  600: Cost      nan   \nIteration  700: Cost      inf   \nIteration  800: Cost      nan   \nIteration  900: Cost      nan   \nIteration  999: Cost      inf   \n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a name=\"3.8\"></a>\n### 3.8 Evaluating regularized logistic regression model\n\nWe will use the `predict` function that you implemented above to calculate the accuracy of the regulaized logistic regression model on the training set","metadata":{}},{"cell_type":"code","source":"#Compute accuracy on the training set\np = predict(X_train, w, b)\n\nprint('Train Accuracy: %f'%(np.mean(p == Y_train) * 100))","metadata":{"execution":{"iopub.status.busy":"2023-01-30T23:10:43.337719Z","iopub.execute_input":"2023-01-30T23:10:43.338180Z","iopub.status.idle":"2023-01-30T23:10:43.390209Z","shell.execute_reply.started":"2023-01-30T23:10:43.338143Z","shell.execute_reply":"2023-01-30T23:10:43.389077Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Train Accuracy: 84.726562\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: overflow encountered in exp\n  del sys.path[0]\n","output_type":"stream"}]},{"cell_type":"code","source":"#Compute accuracy on the training set\np = predict(X_test, w, b)\n\nprint('Test Accuracy: %f'%(np.mean(p == Y_train) * 100))","metadata":{"execution":{"iopub.status.busy":"2023-01-30T23:10:46.325676Z","iopub.execute_input":"2023-01-30T23:10:46.326093Z","iopub.status.idle":"2023-01-30T23:10:46.366617Z","shell.execute_reply.started":"2023-01-30T23:10:46.326058Z","shell.execute_reply":"2023-01-30T23:10:46.365189Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Test Accuracy: 84.726562\n","output_type":"stream"}]},{"cell_type":"code","source":"print(w)","metadata":{"execution":{"iopub.status.busy":"2023-01-30T23:10:50.443867Z","iopub.execute_input":"2023-01-30T23:10:50.444276Z","iopub.status.idle":"2023-01-30T23:10:50.450401Z","shell.execute_reply.started":"2023-01-30T23:10:50.444240Z","shell.execute_reply":"2023-01-30T23:10:50.449349Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"[ 0.01258053  1.11662234 -0.8664392  -0.20937463  1.05313171 -0.3851282\n -0.30538818  0.03272898 -0.06031482 -1.10330875  0.83817804 -1.55531095\n -1.04524531 -2.22059761  0.33945081]\n","output_type":"stream"}]}]}